{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 3: Non-Linear Models and Validation Metrics (37 total marks)\n",
    "### Due: October 24 at 11:59pm\n",
    "\n",
    "### Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses non-linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf275ca7",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b67a661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee2d2c3",
   "metadata": {},
   "source": [
    "## Part 1: Regression (14.5 marks)\n",
    "\n",
    "For this section, we will be continuing with the concrete example from yellowbrick. You will need to compare these results to the results from the previous assignment. Please use the results from the solution if you were unable to complete Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219f163",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (0.5 marks)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the concrete dataset into the feature matrix `X` and target vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2af8bd32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TO DO: Import concrete dataset from yellowbrick library\n",
    "from yellowbrick.datasets import load_concrete\n",
    "X, y = load_concrete()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea4cc",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0 marks)\n",
    "\n",
    "Data processing was completed in the previous assignment. No need to repeat here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a245d00",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import the Decision Tree, Random Forest and Gradient Boosting Machines regression models from sklearn\n",
    "2. Instantiate the three models with `max_depth = 5`. Are there any other parameters that you will need to set?\n",
    "3. Implement each machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dbff22f-1eb8-4f34-91d7-07df5bafb4f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(max_depth=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(max_depth=5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(max_depth=5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import all models\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#Instantiate all three models with max_Depth = 5\n",
    "decisonTree = DecisionTreeRegressor(max_depth = 5)\n",
    "\n",
    "#Need to set random_state since random forest is already random\n",
    "randomForest = RandomForestRegressor(max_depth = 5, random_state = 0)\n",
    "\n",
    "gradientBoosting = GradientBoostingRegressor(max_depth = 5)\n",
    "\n",
    "#Implement each machine learning model with X and y\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "decisonTree.fit(X_train, y_train)\n",
    "randomForest.fit(X_train, y_train)\n",
    "gradientBoosting.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f994e31",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the average training and validation accuracy using mean squared error with cross-validation. To do this, you will need to set `scoring='neg_mean_squared_error'` in your `cross_validate` function and negate the results (multiply by -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17446ebb-d72d-4e69-8b1c-f4abde39f5b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "#MSE mean with cross-validation for Decison Tree\n",
    "decisonTree_scores = cross_validate(decisonTree, X_train, y_train, cv = 5, scoring = 'neg_mean_squared_error',\n",
    "                                    return_train_score = True)\n",
    "decisonTree_train_score = decisonTree_scores['train_score'].mean() * -1\n",
    "decisonTree_test_score = decisonTree_scores['test_score'].mean() * -1\n",
    "\n",
    "#MSE mean with cross-validation for Random Forest\n",
    "randomForest_scores = cross_validate(randomForest, X_train, y_train, cv = 5, scoring = 'neg_mean_squared_error',\n",
    "                                    return_train_score = True)\n",
    "randomForest_train_score = randomForest_scores['train_score'].mean() * -1\n",
    "randomForest_test_score = randomForest_scores['test_score'].mean() * -1\n",
    "\n",
    "#MSE mean for cross-validation for Gradient Boosting\n",
    "gradientBoosting_scores = cross_validate(gradientBoosting, X_train, y_train, cv = 5, scoring = 'neg_mean_squared_error',\n",
    "                                    return_train_score = True)\n",
    "gradientBoosting_train_scores = gradientBoosting_scores['train_score'].mean() * -1\n",
    "gradientBoosting_test_scores = gradientBoosting_scores['test_score'].mean() * -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc3f7a8",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: DT, RF and GB\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdc93a78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DT</th>\n",
       "      <td>47.279761</td>\n",
       "      <td>71.444536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>29.577455</td>\n",
       "      <td>45.059351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB</th>\n",
       "      <td>3.379440</td>\n",
       "      <td>23.144381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Accuracy  Validation Accuracy\n",
       "Model                                        \n",
       "DT             47.279761            71.444536\n",
       "RF             29.577455            45.059351\n",
       "GB              3.379440            23.144381"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.DataFrame(columns = ['Model', 'Training Accuracy', 'Validation Accuracy'])\n",
    "results['Model'] = ['DT', 'RF', 'GB']\n",
    "results.set_index(['Model'], inplace = True)\n",
    "\n",
    "results['Training Accuracy'] = [decisonTree_train_score,randomForest_train_score, gradientBoosting_train_scores]\n",
    "results['Validation Accuracy'] = [decisonTree_test_score, randomForest_test_score, gradientBoosting_test_scores]\n",
    "display(results)\n",
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31715a9d",
   "metadata": {},
   "source": [
    "Repeat the step above to print the R2 score instead of the mean-squared error. For this case, you can use `scoring='r2'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83539f47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DT</th>\n",
       "      <td>0.834465</td>\n",
       "      <td>0.738958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.896557</td>\n",
       "      <td>0.840927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB</th>\n",
       "      <td>0.988171</td>\n",
       "      <td>0.919405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Accuracy  Validation Accuracy\n",
       "Model                                        \n",
       "DT              0.834465             0.738958\n",
       "RF              0.896557             0.840927\n",
       "GB              0.988171             0.919405"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "#R2 mean with cross-validation for Decison Tree\n",
    "decisonTree_scores = cross_validate(decisonTree, X_train, y_train, cv = 5, scoring = 'r2',\n",
    "                                    return_train_score = True)\n",
    "decisonTree_train_score = decisonTree_scores['train_score'].mean()\n",
    "decisonTree_test_score = decisonTree_scores['test_score'].mean()\n",
    "\n",
    "#R2 mean with cross-validation for Random Forest\n",
    "randomForest_scores = cross_validate(randomForest, X_train, y_train, cv = 5, scoring = 'r2',\n",
    "                                    return_train_score = True)\n",
    "randomForest_train_score = randomForest_scores['train_score'].mean()\n",
    "randomForest_test_score = randomForest_scores['test_score'].mean()\n",
    "\n",
    "#R2 mean for cross-validation for Gradient Boosting\n",
    "gradientBoosting_scores = cross_validate(gradientBoosting, X_train, y_train, cv = 5, scoring = 'r2',\n",
    "                                    return_train_score = True)\n",
    "gradientBoosting_train_scores = gradientBoosting_scores['train_score'].mean()\n",
    "gradientBoosting_test_scores = gradientBoosting_scores['test_score'].mean()\n",
    "\n",
    "#Visualize results \n",
    "results = pd.DataFrame(columns = ['Model', 'Training Accuracy', 'Validation Accuracy'])\n",
    "results['Model'] = ['DT', 'RF', 'GB']\n",
    "results.set_index(['Model'], inplace = True)\n",
    "\n",
    "results['Training Accuracy'] = [decisonTree_train_score,randomForest_train_score, gradientBoosting_train_scores]\n",
    "results['Validation Accuracy'] = [decisonTree_test_score, randomForest_test_score, gradientBoosting_test_scores]\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5257a98",
   "metadata": {},
   "source": [
    "### Questions (6 marks)\n",
    "1. How do these results compare to the results using a linear model in the previous assignment? Use values.\n",
    "1. Out of the models you tested, which model would you select for this dataset and why?\n",
    "1. If you wanted to increase the accuracy of the tree-based models, what would you do? Provide two suggestions.\n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1862fe4-2965-4d66-99c9-d593bd9d7309",
   "metadata": {},
   "source": [
    "1. For assignment 2, using the concrete dataset with a linear model, we got a r2 score of 0.61 for the training score and a validation score of 0.62 for R2. For mean squared errors, we got a training score of 111, and a validation score of 96. Our non-linear models are all better than the linear model we used. A decison tree gave a the least improvement with a R2 of 0.83 and a mean squared error of 47 on the training set. The testing score was less, with a score of 0.74 for R2 and a mean squared error of 71. The best model was the gradient boosted tree. the gradient boosted regression tree gave a R2 of 0.988 and a mean squared error of 3.38 on the training set. A R2 of 0.92 and a mean squared error of 23 for the validation test. The random forest regression model results were between the decison trees and the gradient boosted trees results. As you can see, all non-linear models gave better results than the linear model, with gradient boosted regression tree model being the best.\n",
    "\n",
    "2. I would select the gradient boosted regression tree model. This model gave us the highest training scores with a R2 of 0.988 and a mean squared error of 3.38. It's validation score is only a little behind with a R2 of 0.92 and a mean squared error of 23. With already great results, we can also tune our model even more using parameters such as n_estimaters, learning_rate, and max_depth to get better results. For example, when we compare our R2 scores on the data, we can see we might be overfitting slightly. So we can use a low learning_rate to reduce the mode|ls complexity, and thus reduce the overfitting.\n",
    "\n",
    "3. We can always use max_depth, which is shared across all 3 models. For all 3 models, we can make cases that we are slightly overfitting, and thus limiting the max_depth can give us better accuracy results. For the random forest models we have parameters such as n_estimaters and max_features that can be used to improve accuracy results. Such as increasing n_estimaters for the random forest model will reduce overfitting since we are averaging more trees. Same could be said for max_features. A small max_features reduces overfitting as well. For gradient boosted tree models, we can use n_estimaters, learning_rate to optimize the accuracy results. A small learning_rate and n_estimaters will reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b238f4",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93097bfe",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708cbb5f-4dc8-4600-a270-29f6d9cd4526",
   "metadata": {},
   "source": [
    "1. I-Cheng Yeh, August 2nd/2007, \"Concrete Compressive Strength\", UCI Machine Learning Repository. [Online]. Available : https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength\n",
    "\n",
    "2. I started with data input and imported the dataset into two dataframes. I did not process the data since we did that in the last assignment. Then, I implemented the 3 models, decison tree model, random forest  model, and the gradient boosted tree model. I split the data into training and test sets, and trained the three models. Afterwords, I validated the models by analysing their R2  and mean squared error score for the training and validation tests. Finally, I visualized the results by creating a dataframe that shows all mean squared error training scores and validation scores for all three models. I also visualized the results for the R2 score for all three models.\n",
    "\n",
    "3. I did not use generative AI on this part of the assignment.\n",
    "\n",
    "4. I did not struggle with this part of the assignment. I think having an understanding of all 3 models, before I opened up the assignment helped me a lot in this part of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 2: Classification (17.5 marks)\n",
    "\n",
    "You have been asked to develop code that can help the user classify different wine samples. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (2 marks)\n",
    "\n",
    "The data used for this task can be downloaded from UCI: https://archive.ics.uci.edu/dataset/109/wine\n",
    "\n",
    "Use the pandas library to load the dataset. You must define the column headers if they are not included in the dataset \n",
    "\n",
    "You will need to split the dataset into feature matrix `X` and target vector `y`. Which column represents the target vector?\n",
    "\n",
    "Print the size and type of `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33583c67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 13)\n",
      "(178,)\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import wine dataset\n",
    "#!more \"wine.names\"\n",
    "all_data = pd.read_csv(r'wine.data', names = ['class','Alcohol', 'Malicacid','Ash', 'Alcalinity_of_ash', 'Magnesium', \n",
    "                                              'Total_phenols', 'Flavanoids', 'Nonflavanoid_phenols', 'Proanthocyanins', \n",
    "                                              'Color_intensity', 'Hue', '0D280_0D315_of_diluted_wines', 'Proline'])\n",
    "\n",
    "X = all_data.drop(columns = 'class')\n",
    "y = all_data['class']\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28af110",
   "metadata": {},
   "source": [
    "Print the first five rows of the dataset to inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea266921",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malicacid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity_of_ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total_phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid_phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color_intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>0D280_0D315_of_diluted_wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  Alcohol  Malicacid   Ash  Alcalinity_of_ash  Magnesium  \\\n",
       "0      1    14.23       1.71  2.43               15.6        127   \n",
       "1      1    13.20       1.78  2.14               11.2        100   \n",
       "2      1    13.16       2.36  2.67               18.6        101   \n",
       "3      1    14.37       1.95  2.50               16.8        113   \n",
       "4      1    13.24       2.59  2.87               21.0        118   \n",
       "\n",
       "   Total_phenols  Flavanoids  Nonflavanoid_phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   Color_intensity   Hue  0D280_0D315_of_diluted_wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "display(all_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fc8fe",
   "metadata": {},
   "source": [
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97c6e9dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alcohol                         0\n",
      "Malicacid                       0\n",
      "Ash                             0\n",
      "Alcalinity_of_ash               0\n",
      "Magnesium                       0\n",
      "Total_phenols                   0\n",
      "Flavanoids                      0\n",
      "Nonflavanoid_phenols            0\n",
      "Proanthocyanins                 0\n",
      "Color_intensity                 0\n",
      "Hue                             0\n",
      "0D280_0D315_of_diluted_wines    0\n",
      "Proline                         0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "print(X.isnull().sum())\n",
    "print('\\n')\n",
    "print(y.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070956af",
   "metadata": {},
   "source": [
    "How many samples do we have of each type of wine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b37a6fd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples of type 1:  59\n",
      "Samples of type 2:  71\n",
      "Samples of type 3:  48\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "samples = y.value_counts()\n",
    "samples_1 = samples[1]\n",
    "samples_2 = samples[2]\n",
    "samples_3 = samples[3]\n",
    "print('Samples of type 1: ',samples_1)\n",
    "print('Samples of type 2: ',samples_2)\n",
    "print('Samples of type 3: ',samples_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `SVC` and `DecisionTreeClassifier` from sklearn\n",
    "2. Instantiate models as `SVC()` and `DecisionTreeClassifier(max_depth = 3)`\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f38a6b7-251b-4569-9139-1db546100c5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ravee\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ravee\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import SVC and DecisonTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Instatiate models as SVC() and DecisonTreeClassifier(max_depth = 3)\n",
    "linearSVC = LinearSVC()\n",
    "decisonTree = DecisionTreeClassifier(max_depth = 3)\n",
    "\n",
    "#Implement the machine learning model with X and y\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "linearSVC.fit(X_train, y_train)\n",
    "decisonTree.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0870b0d2",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model \n",
    "\n",
    "Calculate the average training and validation accuracy using `cross_validate` for the two different models listed in Step 3. For this case, use `scoring='accuracy'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "876c4f2d-17cc-43de-8f38-0a965d574653",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ravee\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ravee\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ravee\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ravee\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ravee\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ravee\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ravee\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ravee\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ravee\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ravee\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "linearSVC_scores = cross_validate(linearSVC, X_train, y_train, cv = 5, scoring = 'accuracy', return_train_score = True)\n",
    "linearSVC_train_score = linearSVC_scores['train_score'].mean()\n",
    "linearSVC_test_score =  linearSVC_scores['test_score'].mean()\n",
    "\n",
    "decisonTree_scores = cross_validate(decisonTree, X_train, y_train, cv = 5, scoring = 'accuracy', return_train_score = True)\n",
    "decisonTree_train_score = decisonTree_scores['train_score'].mean()\n",
    "decisonTree_test_score =  decisonTree_scores['test_score'].mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0bbd83",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "#### Step 5.1: Compare Models\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be4b5c0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Size for Training</th>\n",
       "      <th>Data Size for Testing</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>(133, 13)</td>\n",
       "      <td>(45, 13)</td>\n",
       "      <td>0.798801</td>\n",
       "      <td>0.797721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decison Tree Classifier</th>\n",
       "      <td>(133, 13)</td>\n",
       "      <td>(45, 13)</td>\n",
       "      <td>0.994357</td>\n",
       "      <td>0.871795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Data Size for Training Data Size for Testing  \\\n",
       "Model                                                                  \n",
       "SVC                                  (133, 13)              (45, 13)   \n",
       "Decison Tree Classifier              (133, 13)              (45, 13)   \n",
       "\n",
       "                         Training Accuracy  Validation Accuracy  \n",
       "Model                                                            \n",
       "SVC                               0.798801             0.797721  \n",
       "Decison Tree Classifier           0.994357             0.871795  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.DataFrame(columns = ['Model','Data Size for Training','Data Size for Testing', 'Training Accuracy', 'Validation Accuracy'])\n",
    "results['Model'] = ['SVC', 'Decison Tree Classifier']\n",
    "results['Data Size for Training'] = [X_train.shape, X_train.shape]\n",
    "results['Data Size for Testing'] = [X_test.shape, X_test.shape]\n",
    "results.set_index(['Model'], inplace = True)\n",
    "\n",
    "results['Training Accuracy'] = [linearSVC_train_score, decisonTree_train_score]\n",
    "results['Validation Accuracy'] = [linearSVC_test_score, decisonTree_test_score]\n",
    "display(results)\n",
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e17878",
   "metadata": {},
   "source": [
    "#### Step 5.2: Visualize Classification Errors\n",
    "Which method gave the highest accuracy? Use this method to print the confusion matrix and classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44b091a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TO DO: Implement best model\n",
    "# Decison tree gave me the highest accuracy\n",
    "pred_decisonTree = decisonTree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09d21b59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(170.97222222222223, 0.5, 'True Value')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TO DO: Print confusion matrix using a heatmap\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion = confusion_matrix(y_test, pred_decisonTree, labels = [1, 2, 3])\n",
    "\n",
    "sns.heatmap(confusion, xticklabels = [1, 2, 3],  yticklabels =  [1, 2, 3], square = True, annot = True, cbar = False)\n",
    "plt.xlabel('Predicted Value')\n",
    "plt.ylabel('True Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ef95947",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.88      0.93        16\n",
      "           2       0.91      0.95      0.93        21\n",
      "           3       0.89      1.00      0.94         8\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.93      0.94      0.93        45\n",
      "weighted avg       0.94      0.93      0.93        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Print classification report\n",
    "from sklearn.metrics import classification_report\n",
    "class_Report = classification_report(y_test, pred_decisonTree, target_names = ['1', '2', '3'])\n",
    "print(class_Report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf319621",
   "metadata": {},
   "source": [
    "### Questions (6 marks)\n",
    "1. How do the training and validation accuracy change depending on the method used? Explain with values.\n",
    "1. What are two reasons why the support vector machines model did not work as well as the tree-based model?\n",
    "1. How many samples were incorrectly classified in step 5.2? \n",
    "1. In this case, is maximizing precision or recall more important? Why?\n",
    "\n",
    "*YOUR ANSWERS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf742bf-6f32-4d4a-b284-95137dd881df",
   "metadata": {},
   "source": [
    "1. For the decison tree classifier, we got a training score of 0.994 and a validation score of 0.872. For the linear support vector classification we got a training score of 0.799, and a validation score of 0.798. It appears that the decisosn tree model is much better than the SVC model. From looking at the models themselves, it appears that the decison tree classifier is overfitting, a common issue among tree models. For SVC, it appears that the model be underfitting.\n",
    "2. This could be because of scaling. From the attributes we can see a values be high as 1000, and low as 0.1. This is a difference of 10^4, possibly causing a scaling issue. Another possible issue is that this dataset is simply better handled by non-linear models instead of linear models. This particular dataset might be hard to seperate linearly, while decison tree model can partition the model with various nodes.\n",
    "3. 3 Samples.\n",
    "4. Percison is more important in this case. We are trying to find the correct cultivators that each wine comes from. That means we are trying to reduce false positives, or in other words, reduce the amount of wine incorrectly labeled to a particular cultivator. This will cause less damage than recall since recall is used to avoid false negatives. In other words, try to label everything positive even if it is wrong. In this case, recall is useless, since incorrectly labeling something negative doesn't have an adverse effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ff8ae",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e837da",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a1ccd-1e5f-48a7-9b18-72a6ec486bf1",
   "metadata": {},
   "source": [
    "1. S. Aeberhard, M. Forina, June 30th/1991, \"Wine\", UCI Machine Learning Repository. [Online]. Available : https://archive.ics.uci.edu/dataset/109/wine\n",
    "2. I started with data input and imported the dataset. I than determined the feature matrix and the target matrix, and put them into variables X and y. I processed the data by determining if there are any null values. I also calculated the number of samples for each unique classification. Then, I implemented the 2 models, SVC and deciston tree classifier model. I split the data into training and test sets, and trained the two models. Afterwords, I validated the models by analysing their training and validation scores. I visualized the results with a dataframe that showed the data size for each model, as well as the training and validation scores. Finally, I made a confusion matrix and a classification report for the best model.\n",
    "3. I did use chatGPT to help me load in the dataset. I simply asked chatGPT how to add columns to a datasheet that doesn't have any. I also asked chatGPT how to label a confusion matrix. I did need to modify my code to add the columns for the dataset and properly label the confusion matrix. This was because I did not understand how the functions fully worked.\n",
    "4. I did struggle loading the dataset, but after that it was simply following the steps we previous learned. I also struggled with the confusion matrix as well, since I did not know that class labels is supposed to match the label inside the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd7358d",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "*ADD YOUR FINDINGS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eb4b8b-d500-4acf-9e0c-9fb3581cebdf",
   "metadata": {},
   "source": [
    "I found consistently that the tree models have a tendancy to overfit. For example, the decison tree model in the first part had a training score of 0.83 and a validation score of 0.74. The decison tree model in the second part had a validation score of 0.99 and a validation score of 0.88. I do feel like this is a non issue for tree models though, because they have so many parameters to help reduce overfitting such as n_estimaters or max_depth.                             \n",
    "\n",
    "I also find that the random forest model and gradient boosting machine models are much better than decison tree model. For example, the validation scores of random forest and gradient boosting machine in the first part is 0.84 and 0.92, respectively. Compared to 0.74 validation score of decison tree for the first part, these two ensembles models are much better. This makes sense, because random forest and gradient boosting machine are both built on top of decison tree model.\n",
    "\n",
    "SVC also appears to struggle with scaling. For example, looking at the attribute \"Proline\", we get numbers that reach 10^3 magnitudes. Another attribute in the dataset is \"Nonflavanoid_phenols\", which has numbers that are 10^-1. This gives us a difference of 10^4 magitude for the two attributes, which can cause a scaling issue for SVM models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97b6ac",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02067170-1443-4626-ab4d-6857528d4f6d",
   "metadata": {},
   "source": [
    "I liked how I got to see how different each tree model is, and how good random forest and gradient boosting machine models are. I did not dislike much in this assignment. I did find copying each attribute name onto columns parameter when loading in the dataset annoying, but thats about it.\n",
    "\n",
    "I found seeing the difference between the three tree models in the first part interesting. It was also fun talking about how we could improve each model with the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa21e53b",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (3 marks)\n",
    "\n",
    "Repeat Part 2 and compare the support vector machines model used to `LinearSVC(max_iter=5000)`. Does using `LinearSVC` improve the results? Why or why not?\n",
    "\n",
    "Is `LinearSVC` a good fit for this dataset? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30fea72e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ravee\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ravee\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ravee\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ravee\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ravee\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ravee\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ravee\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ravee\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ravee\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ravee\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ravee\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ravee\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Size for Training</th>\n",
       "      <th>Data Size for Testing</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(133, 13)</td>\n",
       "      <td>(45, 13)</td>\n",
       "      <td>0.943625</td>\n",
       "      <td>0.909687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Data Size for Training Data Size for Testing  Training Accuracy  \\\n",
       "0              (133, 13)              (45, 13)           0.943625   \n",
       "\n",
       "   Validation Accuracy  \n",
       "0             0.909687  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "#Import all relevant methods\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "#Instatiate model\n",
    "linearSVC = LinearSVC(max_iter = 5000)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "linearSVC.fit(X_train, y_train)\n",
    "\n",
    "#Check for the accuracy\n",
    "linearSVC_scores = cross_validate(linearSVC, X_train, y_train, cv = 5, scoring = 'accuracy', return_train_score = True)\n",
    "linearSVC_train_score = linearSVC_scores['train_score'].mean()\n",
    "linearSVC_test_score =  linearSVC_scores['test_score'].mean()\n",
    "\n",
    "#Print results\n",
    "\n",
    "results = pd.DataFrame(columns = ['Data Size for Training','Data Size for Testing', 'Training Accuracy', 'Validation Accuracy'])\n",
    "results['Data Size for Training'] = [X_train.shape]\n",
    "results['Data Size for Testing'] = [X_test.shape]\n",
    "\n",
    "results['Training Accuracy'] = [linearSVC_train_score]\n",
    "results['Validation Accuracy'] = [linearSVC_test_score]\n",
    "display(results)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc68a4",
   "metadata": {},
   "source": [
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f9e10b-d9d3-46ef-8723-aaa8222bf92e",
   "metadata": {},
   "source": [
    "It did improve the results by quite a bit. Previously, using the defualt SVC model, we had a training and validation score of 0.799 and 0.798, respectively. For the linearSVC model with 5000 iterations, we got a training score of 0.943 and a validation score of 0.910. A possible reason why we got a better result for the new SVC model is because we set a max_iter to 5000. This allows the model to converge and find the optimal solution. Also using more iterations can allow the model to deal with unscaled data much better since it taking more time to find a better solution.\n",
    "\n",
    "linearSVC is not be a good fit for the dataset because of scaling. The attributes of the datasets can values that differ by magnitudes of 10^4, and that can adversely affect the model. We can fix this by scaling the dataset. Also it's worth noting even with a higher iteration, the SVC model is still lacking behind the decison tree model. A ensemble model will be even better since they handle overfitting much better than a normal decison tree model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
